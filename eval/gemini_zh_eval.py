# -*- coding: utf-8 -*-
import json
import os
import pandas as pd
from PIL import Image, ImageFont, ImageDraw, ImageFilter
from tqdm import tqdm
from multiprocessing import Pool
import ast
import re
from collections import defaultdict
import base64
from mimetypes import guess_type
from openai import OpenAI

import argparse

def local_image_to_data_url(image_path):
    # Guess the MIME type of the image based on the file extension
    mime_type, _ = guess_type(image_path)
    if mime_type is None:
        mime_type = 'application/octet-stream'  # Default MIME type if none is found

    # Read and encode the image file
    with open(image_path, "rb") as image_file:
        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')

    # Construct the data URL
    return f"data:{mime_type};base64,{base64_encoded_data}"

def call_evaluation(args, client):
    index, prompt, testpoint, test_desc, model, img_path = args

    explanation_dict = {
        'å…³ç³»-æ¯”è¾ƒå…³ç³»':'ä¸¤è€…çš„å±æ€§å¯¹æ¯”',
        'å…³ç³»-æ„æˆå…³ç³»':'ä¸€ä¸ªå®ä½“ç”±å¦ä¸€ç§æˆ–å‡ ç§å®ä½“æ„æˆ',
        'å…³ç³»-åŒ…å«å…³ç³»':'å®¹å™¨å¯¹å®ä½“çš„åŒ…å«å…³ç³»ï¼Œå®¹å™¨ä¹Ÿå¯ä»¥æ˜¯å¹³é¢çš„ï¼Œæ¯”å¦‚ï¼šå¢™ä¸Šçš„ç”»é‡Œæœ‰ä¸€åªè›‡',
        'å…³ç³»-ç›¸ä¼¼å…³ç³»':'ä¸åŒå®ä½“ä¸­å­˜åœ¨çš„ç›¸ä¼¼å…³ç³»',
        'å¤åˆè€ƒç‚¹-æƒ³è±¡åŠ›':'ç°å®ç”Ÿæ´»ä¸­ä¸å¯èƒ½å‘ç”Ÿçš„äº‹æƒ…',
        'å¤åˆè€ƒç‚¹-ä¸åŒå®ä½“ç‰¹å¾åŒ¹é…':'ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒç±»çš„å±æ€§ç‰¹å¾',
        'å®ä½“å¸ƒå±€-ä¸‰ç»´ç©ºé—´':'å¯¹äºä¸‰ç»´ç©ºé—´å®ä½“çš„æ‘†æ”¾å¸ƒå±€', 
        'å®ä½“å¸ƒå±€-äºŒç»´ç©ºé—´':'å¯¹äºäºŒç»´ç©ºé—´å®ä½“çš„æ‘†æ”¾å¸ƒå±€', 
        'å±æ€§-å¤§å°':'å¯¹ä¸»ä½“ å¤§å°/é«˜ä½/é•¿çŸ­/ç²—ç»†/å®½çª„/é«˜çŸ®',
        'å±æ€§-è¡¨æƒ…':'åŒºåˆ†è¡¨æƒ…å’Œè„¸éƒ¨åŠ¨ä½œï¼Œè„¸éƒ¨åŠ¨ä½œç»„æˆè¡¨æƒ…ï¼Œä½†è¡¨æƒ…æ˜¯ä¸€å®šè¦ä½“ç°å‡ºæŸç§æƒ…ç»ªçš„ã€‚',
        'å±æ€§-æ•°é‡':'é‡ç‚¹è€ƒå¯Ÿä¸‰ä¸ªæˆ–ä¸‰ä¸ªä»¥ä¸Šçš„æ•°å­—éš¾ç‚¹',
        'å±æ€§-æè´¨':'è€ƒå¯Ÿä¸åŒæè´¨',
        'åŠ¨ä½œ-äººç‰©/æ‹Ÿäººå…¨èº«åŠ¨ä½œ':'äººç‰©æˆ–æ‹Ÿäººå…¨èº«æ€§çš„åŠ¨ä½œï¼Œæ¯”å¦‚å¥”è·‘ã€è·³æ°´ã€è·³è¡—èˆã€è¡ç§‹åƒã€å€’æŒ‚é‡‘é’©ç­‰',
        'åŠ¨ä½œ-äººç‰©/æ‹Ÿäººæ‰‹éƒ¨åŠ¨ä½œ':'é’ˆå¯¹æ‰‹éƒ¨ç»“æ„çš„è€ƒç‚¹ï¼Œè€ƒæ ¸æ‰‹æŒ‡æ˜¯å¦æœ‰ç¼ºå¤±ã€å´©åç­‰é—®é¢˜',
        'åŠ¨ä½œ-åŠ¨ç‰©åŠ¨ä½œ':'åŠ¨ç‰©çš„åŠ¨ä½œ',
        'åŠ¨ä½œ-å®ä½“é—´æœ‰æ¥è§¦äº’åŠ¨':'å„ç§å®ä½“é—´çš„æœ‰æ¥è§¦äº’åŠ¨',
        'åŠ¨ä½œ-å®ä½“é—´æ— æ¥è§¦äº’åŠ¨':'æ¯”å¦‚ä¸¤ä¸ªäººå¯¹è§†ï¼Œè€ƒæ ¸æ¨¡å‹èƒ½å¦æŠŠå¯¹è§†å…³ç³»ç”»å¯¹',
        'åŠ¨ä½œ-çŠ¶æ€':'å®ä½“æŒç»­çš„çŠ¶æ€ï¼Œä¸€èˆ¬æ˜¯ä¸€ä¸ªåŠ¨è¯ã€‚',
        'è¯­æ³•-å¦å®š':'è€ƒå¯Ÿæ¨¡å‹å¯¹äºå¦å®šè¯­æ³•çš„æŒæ¡ç¨‹åº¦',
        'è¯­æ³•-ä»£è¯æŒ‡ä»£':'è¿™é‡Œçš„ä»£è¯é€šå¸¸æ˜¯æœ‰ä¸€äº›è¿·æƒ‘æ€§çš„ï¼Œè€ƒå¯Ÿæ¨¡å‹èƒ½å¦æ­£ç¡®å¯¹åº”',
        'è¯­æ³•-ç»Ÿä¸€æ€§':'å®ä½“å…±åŒå±æ€§çš„è€ƒå¯Ÿ',
        'ä¸–ç•ŒçŸ¥è¯†':'åäººã€å»ºç­‘ã€åŸºç¡€çš„é¢†åŸŸçŸ¥è¯†ã€ç½‘ç»œæµè¡Œè¯­ã€‚å…¶ä¸­åäººä¸è¦ä½¿ç”¨å½“ä»£æœ‰ç‰ˆæƒé£é™©çš„åäºº',
        'é£æ ¼':'è‰ºæœ¯ã€ç»˜ç”»ã€æ‘„å½±ã€è®¾è®¡é£æ ¼ï¼ŒåŠå¯¹åº”è‰ºæœ¯å®¶åç§°',
        'é€»è¾‘æ¨ç†': 'éœ€è¦æ¨¡å‹æ·±å…¥ç†è§£æ„å›¾å¹¶è¿›è¡Œä¸€å®šçš„æ¨ç†',
        'æ–‡æœ¬ç”Ÿæˆ':'é•¿ã€çŸ­æ–‡å­—ï¼Œå­—ä½“ã€æ–‡å­—è½½ä½“',
        }

    explanation = "è€ƒç‚¹è¯´æ˜ï¼šã€Œã€Œ"

    for point in testpoint:
        if point not in explanation_dict:
            print(f'{point} do not exist!')
            raise()

        explanation += f"\n{point}: {explanation_dict[point]}"
    explanation += "\nã€"

    test_explanation = "è€ƒç‚¹æè¿°è¯´æ˜ï¼šã€Œ"
    for idx, point in enumerate(testpoint):
        test_explanation += f"\n{point}: {test_desc[idx]}"
    test_explanation += "\nã€"


    while True:
        system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®ä¸”å®¢è§‚çš„ä¸­æ–‡å›¾åƒæè¿°ç³»ç»Ÿã€‚æˆ‘ä¼šç»™ä½ ä¸€æ®µç”Ÿæˆå›¾åƒçš„æç¤ºè¯ï¼Œä»¥åŠå¯¹åº”çš„ç”Ÿæˆå›¾åƒï¼ŒåŒæ—¶å¯¹äºç”Ÿæˆå›¾åƒä¸æç¤ºè¯ä¹‹é—´ç›¸å…³æ€§çš„è€ƒç‚¹åŠå¯¹åº”è¯´æ˜ï¼Œä½ éœ€è¦é€ä¸ªè€ƒç‚¹æ¥åˆ¤æ–­ç”Ÿæˆçš„å›¾åƒæ˜¯å¦éµä»äº†æç¤ºè¯ä¸­æ‰€åŒ…å«çš„å¯¹åº”è€ƒç‚¹è¦æ±‚ã€‚

        é’ˆå¯¹æ¯å¼ å›¾åƒï¼Œä½ éœ€è¦æŒ‰ç…§é¡ºåºå®Œæˆå¦‚ä¸‹çš„ä»»åŠ¡ï¼š
        1. è¿™å¼ ç”Ÿæˆå›¾åƒå¯¹åº”çš„æç¤ºè¯ä¸ºã€Œ{prompt}ã€ï¼Œä½ éœ€è¦æ ¹æ®{testpoint}ä¸­çš„è¿™äº›è§’åº¦é€ä¸ªå¯¹å›¾åƒå†…å®¹è¿›è¡Œæ›´è¿›ä¸€æ­¥çš„è¯¦ç»†åˆ†æï¼Œè€ƒç‚¹çš„è¯¦ç»†è¯´æ˜å¦‚ä¸‹ï¼š{explanation}ï¼Œå„ä¸ªè€ƒç‚¹åœ¨è¿™æ¡promptä¸­å¯¹åº”çš„æè¿°è¯´æ˜å¦‚ä¸‹ï¼š{test_explanation}, ä½ éœ€è¦æ ¹æ®è€ƒç‚¹é€ä¸€åˆ¤æ–­ç”Ÿæˆå›¾åƒæ˜¯å¦æ»¡è¶³äº†è€ƒç‚¹å¯¹åº”çš„è¦æ±‚
        2. ç»¼åˆä¸Šè¿°å›ç­”ï¼Œä½ éœ€è¦é€ä¸ªè€ƒç‚¹åˆ¤æ–­ç”Ÿæˆçš„å›¾åƒåœ¨è€ƒç‚¹å…³æ³¨ç»´åº¦ä¸Šæ˜¯å¦ç¬¦åˆè¾“å…¥çš„promptï¼Œå¦‚æœæ»¡è¶³è¦æ±‚åˆ™è¯¥è€ƒç‚¹å¾—åˆ†ä¸º1ï¼Œå¦åˆ™ä¸º0

        çº¦æŸæ¡ä»¶ï¼š
        - ä»…æè¿°ç›´æ¥å¯è§çš„å†…å®¹ï¼›ä¸è¦è¿›è¡Œè§£è¯»ã€æ¨æµ‹æˆ–æš—ç¤ºèƒŒæ™¯æ•…äº‹ã€‚
        - ä¸“æ³¨äºèƒ½å¤Ÿç¡®å®šæ€§é™ˆè¿°çš„è§†è§‰ç»†èŠ‚ã€‚
        - çœç•¥ä¸ç¡®å®šæˆ–ä¸æ¸…æ™°çš„ç»†èŠ‚ã€‚
        - å³ä½¿è¾“å…¥ä¸­å­˜åœ¨ï¼Œä¹Ÿä¸è¦æè¿°æŠ½è±¡å®ä½“ã€æƒ…æ„Ÿæˆ–æ¨æµ‹ã€‚

        è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¾“å‡ºæ ¼å¼ï¼š

        <description>
            <prompt>{prompt}</prompt>
            <checkpoint>{testpoint}</checkpoint>
            <analysis>æŒ‰ç…§æ­¥éª¤1å¯¹äºç»™å®šè€ƒç‚¹è¿›è¡Œé€é¡¹è¯¦ç»†åˆ†æï¼Œæ ¼å¼ä¸ºä¸€ä¸ªæ–¹æ‹¬å·åˆ—è¡¨ï¼Œ**ç¡®ä¿åˆ—è¡¨çš„é•¿åº¦ä¸è€ƒç‚¹çš„æ•°é‡ç›¸ç­‰**ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºå¯¹äºå¯¹åº”è€ƒç‚¹çš„åˆ†æ</analysis>
            <score>æŒ‰ç…§æ­¥éª¤2é€ä¸ªå¯¹è€ƒç‚¹è¿›è¡Œæ‰“åˆ†ï¼Œæ ¼å¼ä¸ºä¸€ä¸ªæ–¹æ‹¬å·åˆ—è¡¨ï¼Œ**ç¡®ä¿åˆ—è¡¨çš„é•¿åº¦ä¸è€ƒç‚¹çš„æ•°é‡ç›¸ç­‰**ï¼Œæ¯ä¸ªå…ƒç´ ä¸º0æˆ–è€…1ï¼Œè¡¨ç¤ºå¯¹åº”è€ƒç‚¹æ˜¯å¦å®Œæˆ</score>
        </description>
        '''
        


        base64_image = local_image_to_data_url(img_path)

        response = client.chat.completions.create(
                model="gemini-2.5-pro", 
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": system_prompt},
                            {"type": "image_url", "image_url": {"url": f"{base64_image}"}},
                        ],
                    },
                ],
        )
        text = response['choices'][0]['message']['content']
        # text = response.choices[0].message.content
        print(text)


        if text is not None:
            try:
                analysis_match = re.search(r'<analysis>(.*?)</analysis>', text, re.DOTALL)
                score_match = re.search(r'<score>(.*?)</score>', text, re.DOTALL)

                analysis_str = analysis_match.group(1).strip()
                analysis = ast.literal_eval(analysis_str)
                
                score_str = score_match.group(1).strip()
                score = ast.literal_eval(score_str)

                if len(testpoint) != len(analysis) or len(testpoint) != len(score):
                    continue
            except Exception as e:
                print(e)
                continue

            result_json = {
                'prompt': prompt,
                'testpoint': testpoint,
                'analysis': analysis,
                'score': score,
            }


            return dict(
                index = index,
                testpoint = testpoint,
                prompt = prompt,
                img_path=img_path,
                output = text,
                result_json = result_json,
            )
        else:
            print("None")
            continue


def main(data_path: str, api_key: str, base_url: str, csv_file: str):
    model = "gemini-2.5-pro"

    client = OpenAI(
            api_key=api_key 
            base_url=base_url
        )

    file_name = data_path.split('/')[-1]


    out_file = f'./results/{file_name}_zh.csv'
    os.makedirs(os.path.dirname(out_file), exist_ok=True)

    if os.path.exists(out_file):
        os.remove(out_file) 
        print(f"remove existing file: {out_file}")
    suffix = '.png'

    df = pd.read_csv(csv_file)
    df['index'] = df['index'].apply(lambda x: int(x))

    args = []
    for i in tqdm(range(len(df)), total=len(df)):
        index = df.iloc[i]['index']
        
        prompt = df.iloc[i]['prompt_zh']
        subdim_dicts = df.iloc[i]['sub_dims_zh']

        test_point = json.loads(subdim_dicts)['è€ƒç‚¹']
        test_desc = json.loads(subdim_dicts)['è€ƒç‚¹å¯¹åº”æè¿°']

        for j in range(4):
            img_path = os.path.join(data_path, f"{index}_{j}{suffix}")

            if not os.path.exists(img_path):
                raise()
                    
            args.append((index, prompt, test_point, test_desc, model, img_path))

    # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†
    pool = Pool(processes=20)
    try:
        for result in tqdm(pool.imap(call_evaluation, args, client), total=len(args)):
  
            new_row = pd.DataFrame([{
                'index': str(int(result['index'])),
                'prompt': result['prompt'],
                'testpoint': str(result['testpoint']),
                'output': result['output'],
                'result_json': json.dumps(result['result_json'], ensure_ascii=False, indent=4),
                'img_path': result['img_path']
            }])

            if not os.path.exists(out_file):
                new_row.to_csv(out_file, index=False)
            else:
                existing_df = pd.read_csv(out_file)
                updated_df = pd.concat([existing_df[existing_df['index'] != str(result['index'])], new_row])
                updated_df.to_csv(out_file, index=False)
            
    finally:
        pool.close()
        pool.join()

    print(f"Finished! Evaluation results are saved to: {out_file}")

    # Calculate scores
    df = pd.read_csv(out_file)

    big_class_stats = defaultdict(lambda: [0, 0])  
    small_class_stats = defaultdict(lambda: [0, 0]) 

    for _, row in df.iterrows():
        checkpoints = ast.literal_eval(row['testpoint'])
        scores = ast.literal_eval(row['result_json'])['score'] if isinstance(row['result_json'], str) else row['score']

        if not isinstance(scores, list):
            scores = ast.literal_eval(row['score'])

        for cp, score in zip(checkpoints, scores):

            if '-' in cp:
                big_class, small_class = cp.split('-', 1)[0], cp
            else:
                big_class = small_class = cp

            big_class_stats[big_class][1] += 1
            small_class_stats[small_class][1] += 1
            if score == 1:
                big_class_stats[big_class][0] += 1
                small_class_stats[small_class][0] += 1

    print("ğŸ“˜ Primary Dimension Evaluation Results:")
    for big_class, (correct, total) in big_class_stats.items():
        acc = correct / total if total > 0 else 0
        print(f"  - {big_class}: {correct}/{total} = {acc:.2%}")

    print("\nğŸ“— Sub Dimension Evaluation Results:")
    for small_class in sorted(small_class_stats.keys()):
        correct, total = small_class_stats[small_class]
        acc = correct / total if total > 0 else 0
        print(f"  - {small_class}: {correct}/{total} = {acc:.2%}")



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluation with Gemini2.5-pro")
    parser.add_argument("--data_path", type=str, required=True, help="Directory to save generated images")
    parser.add_argument("--api_key", type=str, required=True, help="API key for OpenAI")
    parser.add_argument("--base_url", type=str, required=True, help="Base URL for OpenAI API")
    parser.add_argument("--csv_file", type=str, default="data/test_prompts_en.csv", help="CSV file containing prompts")

    args = parser.parse_args()
    main(args.data_path, args.api_key, args.base_url, args.csv_file)